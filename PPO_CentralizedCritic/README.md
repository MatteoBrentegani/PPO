# PPO with Centralized Critic

# Summary
This code implement a multiple agent version of PPO with centralized critic. 
Below is an example of simulation on the environment with a single shared target.

<img src="https://github.com/MatteoBrentegani/PPO/blob/master/PPO_CentralizedCritic/MA_1Goal001.gif" width="250" height="250"/>

# Code
* ``main.py``

Create environmentt and run the ppo algorithm.

* ``config.yaml``

Configuration about agent and environment

* ``ppo_V2.py``

Implementation of PPO algorithm.


### performance examples

## Agent 1
![image](https://github.com/MatteoBrentegani/PPO/blob/master/PPO_CentralizedCritic/results/Agent1.png)
## Agent 2
![image](https://github.com/MatteoBrentegani/PPO/blob/master/PPO_CentralizedCritic/results/Agent2.png)
## Agent 3
![image](https://github.com/MatteoBrentegani/PPO/blob/master/PPO_CentralizedCritic/results/Agent3.png)
## Agent 4
![image](https://github.com/MatteoBrentegani/PPO/blob/master/PPO_CentralizedCritic/results/Agent4.png)
